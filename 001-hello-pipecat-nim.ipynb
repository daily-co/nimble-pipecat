{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db60caf-a890-4e62-8255-62fd691cd6e6",
   "metadata": {},
   "source": [
    "# Voice Agent Framework for Conversational AI\n",
    "In this notebook, we walk through how to craft and deploy a voice AI agent using **[Pipecat AI](https://github.com/pipecat-ai/pipecat)**. We illustrate the basic Pipecat flow with the `meta/llama-3.3-70b-instruct` LLM model (set in Step 3) and Riva for STT (Speech-To-Text) & TTS (Text-To-Speech). However, Pipecat is not opinionated and other models and STT/TTS services can easily be used. See [Pipecat documentation](https://docs.pipecat.ai/server/services/supported-services#supported-services) for other supported services.\n",
    "\n",
    "Pipecat AI is an open-source framework for building voice and multimodal conversational agents. Pipecat simplifies the complex voice-to-voice AI pipeline, and lets developers build AI capabilities easily and with Open Source, commercial, and custom models. See [Pipecat's Core Concepts](https://docs.pipecat.ai/getting-started/core-concepts) for a deep dive into how it works.\n",
    "\n",
    "The framework was developed by Daily, a company that has provided real-time video and audio communication infrastructure since 2016. It is fully vendor neutral and is not tightly coupled to Daily's infrastructure. That said, we do use it in this demo.\n",
    "\n",
    "Below is the architecture diagram:\n",
    "\n",
    "![Architecture Diagram](https://raw.githubusercontent.com/dglogo/nimble-pipecat/main/arch.png)\n",
    "\n",
    "A three-phase approach is used for Conversational AI Agent with Pipecat and NVIDIA NIM:\n",
    "\n",
    "#### Phase 1 : User Input\n",
    "- Audio Processing with NVIDIA RIVA ASR with NIM\n",
    "\n",
    "#### Phase 2: User Content Aggregator with Pipecat and NVIDIA NIM\n",
    "- Custom processing with Pipecat\n",
    "- NVIDIA RIVA TTS with NIM\n",
    "\n",
    "#### Phase 3: Run the Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb5b93",
   "metadata": {},
   "source": [
    "# Content Overview \n",
    "\n",
    "- [Prerequisites](#prerequisites)\n",
    "- [Initialize the User Input](#initialize-the-user-input)\n",
    "- [Initialize the Content Aggragtor](#initialize-the-context-aggregator) \n",
    "- [Run the Agent](#run-the-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4fa7d7-88fb-4b33-8145-ee1a91e58af1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Prior to getting started, you will need to create an API Key for the NVIDIA API Catalog and a Daily API Key for the voice agent's transport layer in this demo.\n",
    "\n",
    "### Obtain API Keys\n",
    "#### NGC API Key\n",
    "- NVIDIA API Catalog\n",
    "  1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**.\n",
    "  2. Select any model, such as `llama-3.3-70b-instruct`.\n",
    "  3. On the right panel above the sample code snippet, click on \"Get API Key\". This will prompt you to log in if you have not already.\n",
    "\n",
    "#### Daily API Key\n",
    "1. Signup at **[Daily](https://dashboard.daily.co/u/signup?pipecat=y)**.\n",
    "2. Verify email address and choose a subdomain to complete onboarding.\n",
    "3. Click on \"Developers\" in left-side menu of Daily dashboard to reveal API Key.\n",
    "\n",
    "### Export API Keys\n",
    "Save these API Keys as environment variables.\n",
    "\n",
    "First, set the NVIDIA API Key as an environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca33825-2191-4fd7-b3c8-514959e15186",
   "metadata": {},
   "source": [
    "Now set the Daily API Key as an environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2ff4e7-b65d-4961-b23f-5b83e330147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"DAILY_API_KEY\", \"\"):\n",
    "    daily_key = getpass.getpass(\"Enter your DAILY API key: \")\n",
    "    assert len(daily_key) == 64, f\"{daily_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"DAILY_API_KEY\"] = daily_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cf235",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "First we set our environment.\n",
    "\n",
    "We use Daily for transport, OpenAI for context aggregation, Riva for TTS & TTS, and Silero for VAD (Voice Activity Detection). If using different services, for example Cartesia for TTS, one would run `pip install \"pipecat-ai[cartesia]\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d7f76-bb78-4614-ab77-229ed3eea402",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"pipecat-ai[daily,openai,riva,silero]\"\n",
    "!pip install noaa_sdk #for function calling example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979c5d1-97a9-42e7-9de2-88b7d31b1409",
   "metadata": {},
   "source": [
    "## Initialize the User Input\n",
    "\n",
    "Create Daily room, where we will navigate to to talk to our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1be9b-e052-4430-b7e1-d7bf57a5ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import os\n",
    "\n",
    "from pipecat.transports.services.helpers.daily_rest import DailyRESTHelper, DailyRoomParams\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    daily_rest_helper = DailyRESTHelper(\n",
    "        daily_api_key=os.getenv(\"DAILY_API_KEY\"),\n",
    "        daily_api_url=os.getenv(\"DAILY_API_URL\", \"https://api.daily.co/v1\"),\n",
    "        aiohttp_session=session,\n",
    "    )\n",
    "\n",
    "    room_config = await daily_rest_helper.create_room(\n",
    "        DailyRoomParams(properties={\"enable_prejoin_ui\":False})\n",
    "    )\n",
    "    DAILY_ROOM_URL = room_config.url\n",
    "\n",
    "    # Url to talk to the NVIDIA NIM Agent\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    print(f\"At the 'Run the Agent!' step, navigate to: {DAILY_ROOM_URL}\")\n",
    "    print(\"\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd23b85-7f53-4564-a52a-dfe29842dbfa",
   "metadata": {},
   "source": [
    "Configure Daily transport for WebRTC communication\n",
    "- DAILY_ROOM_URL: Where to connect (and where we will navigate to to talk to our agent)\n",
    "- None: No authentication token needed\n",
    "- Agent name\n",
    "- Daily params regarding VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a36eb3-38fc-457b-9916-b5ceac796671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.audio.vad.silero import SileroVADAnalyzer\n",
    "from pipecat.transports.services.daily import DailyParams, DailyTransport\n",
    "\n",
    "transport = DailyTransport(\n",
    "    DAILY_ROOM_URL,\n",
    "    None,\n",
    "    \"Lydia\",\n",
    "    DailyParams(\n",
    "        audio_out_enabled=True,\n",
    "        vad_enabled=True,\n",
    "        vad_analyzer=SileroVADAnalyzer(),\n",
    "        vad_audio_passthrough=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506527e-b84c-49e1-8af4-223fdb33f582",
   "metadata": {},
   "source": [
    "### Initialize the LLM and RIVA services with NVIDIA NIM\n",
    "\n",
    "You can customize the LLM `model` as well as the RIVA ASR and TTS services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2cecf",
   "metadata": {},
   "source": [
    "### Working with the NVIDIA API Catalog\n",
    "\n",
    "In this notebook, you will use the newest llama model `llama-3.3-70b-instruct` as the LLM. Define the LLM below and test the API Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d77d5-c183-43d0-980d-fd99a2836365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pipecat.services.nim import NimLLMService\n",
    "from pipecat.services.riva import FastPitchTTSService, ParakeetSTTService\n",
    "\n",
    "stt = ParakeetSTTService(api_key=os.getenv(\"NVIDIA_API_KEY\"))\n",
    "\n",
    "llm = NimLLMService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), model=\"meta/llama-3.3-70b-instruct\"\n",
    ")\n",
    "\n",
    "tts = FastPitchTTSService(api_key=os.getenv(\"NVIDIA_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03d18e",
   "metadata": {},
   "source": [
    "### Optional: Locally Run NVIDIA NIM Microservices\n",
    "\n",
    "Once you familiarize yourself with this blueprint, you may want to self-host models with NVIDIA NIM Microservices using NVIDIA AI Enterprise software license. This gives you the ability to run models anywhere, giving you ownership of your customizations and full control of your intellectual property (IP) and AI applications.\n",
    "Pipecat allows you to pass in a `base_url` to use the local NIM Microservice.\n",
    "\n",
    "[Learn more about NIM Microservices](https://developer.nvidia.com/blog/nvidia-nim-offers-optimized-inference-microservices-for-deploying-ai-models-at-scale/)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE:</b> Run the following cell **ONLY** if you're using a local NIM Microservice instead of the API Catalog Endpoint.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ffd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pipecat.services.nim import NimLLMService\n",
    "\n",
    "llm = NimLLMService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), base_url=\"http://localhost:8000/v1\", model=\"meta/llama-3.1-70b-instruct\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac150732-cbb4-4c70-8d31-cab5ae51b5fb",
   "metadata": {},
   "source": [
    "### Define LLM prompt\n",
    "\n",
    "Edit the prompt as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d884775-c4c0-49eb-b502-d4c855cc8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"\n",
    "You are Lydia; a conversational voice agent who discusses Nvidia's work in agentic AI and a sales assistant who listens to the user and answers their questions. The purpose is to show that voice agents can talk naturally in open-ended conversation. If you are asked how you were built, say you were built with the pipe cat framework and the in vidia NIM platform.\n",
    "\n",
    "Here is background content to reference in the conversation. Only use the background content provided.\n",
    "\n",
    "BACKGROUND:\n",
    "\n",
    "NVIDIA stands at the forefront of the AI revolution, driving major advancements through its comprehensive hardware and software ecosystem.\n",
    "\n",
    "Specific areas of innovation and partnership include:\n",
    "  - healthcare\n",
    "  - customer service\n",
    "  - supercomputers\n",
    "  - scientific research\n",
    "  - manufacturing and automation\n",
    "\n",
    "The company's influence extends beyond traditional GPU manufacturing to pioneering roles in agentic AI, multistep reasoning, and data center architectures, particularly through technologies like NVIDIA NVLink that enable seamless communication among thousands of accelerators.\n",
    "\n",
    "In the customer service sector, NVIDIA is transforming interactions through AI agents powered by NIM microservices and NeMo Retriever. These solutions enable sophisticated natural language processing, retrieval-augmented generation, and digital human interfaces with real-time lip syncing. Global partners including Accenture, Dell Technologies, and Lenovo are leveraging NVIDIA Blueprints to deploy AI solutions across various applications, from warehouse safety to traffic management.\n",
    "\n",
    "NVIDIA's impact is particularly notable in Japan, where collaborations with major providers like SoftBank Corp. and KDDI are establishing AI data centers nationwide. The company's AI Enterprise and Omniverse platforms are enabling Japanese companies to develop culturally-specific language models and enhance industrial automation, with applications ranging from healthcare to manufacturing.\n",
    "\n",
    "In healthcare, NVIDIA is partnering with organizations like Deloitte to improve patient experiences through AI-driven platforms. The company's technologies are being utilized by institutions such as the National Cancer Institute for drug discovery and medical imaging advancement. Additionally, NVIDIA is working with U.S. technology leaders to integrate its AI software into various sectors, with consulting firms like Accenture and cloud providers like Google Cloud facilitating rapid deployment of AI workloads.\n",
    "\n",
    "CRITICAL VOICE REQUIREMENTS:\n",
    "\n",
    "Your responses will be converted to audio. Please do not include any special characters in your response other than '!' or '?'. never use '*'. Replace \"NVIDIA\" with \"in vidia\" and replace \"GPU\" with \"gee pee you\" in your responses. Also, replace \"U.S.\" with \"united states\" and replace \"US\" with \"united states\". Replace \"API\" with \"A pee eye\" and \"AI-driven\" with \"AI driven\".\n",
    "\n",
    "RESPONSE REQUIREMENTS:\n",
    "\n",
    "Speaking style:\n",
    "- You are a realtime voice agent - keep responses natural but brief\n",
    "- Begin with one clear point about what the user asked\n",
    "- If needed, add one or two follow-up details that adds value\n",
    "- Then ask a question to move the conversation forward\n",
    "- Never repeat or rephrase information\n",
    "- Never repeat questions verbatim\n",
    "- Never explain the same concept twice\n",
    "- Never restate what the user just said\n",
    "- Avoid connector phrases like also, additionally, furthermore, moreover\n",
    "\n",
    "Example of BAD response (too long):\n",
    "\"In vidia's agentic AI helps with customer service by reducing wait times and improving satisfaction. The system uses natural language processing to understand customer needs. It can handle multiple languages and complex queries. The AI agents can scale to handle increasing demand. What aspects interest you?\"\n",
    "\n",
    "Example of BAD response (too short):\n",
    "\"In vidia's AI helps customers. What interests you?\"\n",
    "\n",
    "Example of GOOD response:\n",
    "\"In vidia's agentic AI reduces customer wait times by eighty percent through automated response handling. Our recent deployment at The Ottawa Hospital showed significant improvements in patient satisfaction. What specific outcomes would you like to achieve for your customers?\"\n",
    "\n",
    "Example of BAD response:\n",
    "\"In vidia's agentic AI helps with customer service. As I mentioned, it can handle customer inquiries. What interests you about customer service?\"\n",
    "\n",
    "Example of GOOD response:\n",
    "\"In vidia's agentic AI reduces customer wait times by eighty percent. What aspects of customer service interest you?\"\n",
    "\n",
    "Natural Acknowledgments:\n",
    "- Use brief, natural acknowledgments like \"That's interesting\" or \"Great question\" when appropriate\n",
    "- Keep acknowledgments professional and brief\n",
    "- Focus on the topic, not emotional support\n",
    "- Avoid overly familiar phrases like \"no worries\" or \"you're doing great\"\n",
    "\n",
    "Example of BAD response:\n",
    "\"That's wonderful! You're asking such great questions. In vidia's AI...\"\n",
    "\n",
    "Example of GOOD response:\n",
    "\"Interesting point about automation. In vidia's AI reduces processing time by sixty percent. What aspects of efficiency are most important to your team?\"\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "You can:\n",
    "  - Answer questions about in vidia's work in agentic AI\n",
    "  - Discuss the impact of in vidia's AI solutions on various industries\n",
    "  - Provide weather information for anywhere in the United States\n",
    "\n",
    "You cannot:\n",
    "  - Provide weather information for locations outside the United States\n",
    "\n",
    "If you are asked about a location outside the United States, politely respond that you are only able to retrieve current weather information for locations in the United States. If a location is not provided, always ask the user what location for which they would like the weather.\n",
    "\n",
    "After responding to the first question about the weather, ask the user if they'd like to continue with weather questions or talk about in vidia. Reference the most recent conversational context regarding in vidia, if there is any.\n",
    "\n",
    "Now introduce yourself to user by saying \"Hello, I'm Lydia. I'm looking forward to talking about in vidia's recent work in agentic AI. I can also demonstrate tool use by responding to questions about the current weather anywhere in the United States. Who am I speaking with?\" \n",
    "\n",
    "If the user introduces themself, respond with \"Nice to meet you. Is there an agentic use case you're interested in, or a particular industry?\"\n",
    "\n",
    "If the user does not introduce themself, simply continue with the conversation.\n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923291b-4823-46f9-88f6-e88b42bf3191",
   "metadata": {},
   "source": [
    "### Define tool calling function for weather queries\n",
    "\n",
    "Here we use the classic \"get_weather\" example. We use OpenAI's ChatCompletionToolParam and register the function with the llm. Note: this is currently using the `meta/llama-3.3-70b-instruct` model. Not all models support tool calling, so be sure to check this capability before changing or updating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e13ce0-5077-4ccc-be54-12932d816542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletionToolParam\n",
    "from noaa_sdk import NOAA\n",
    "\n",
    "async def start_fetch_weather(function_name, llm, context):\n",
    "    print(f\"Starting fetch_weather_from_api with function_name: {function_name}\")\n",
    "\n",
    "async def get_noaa_simple_weather(latitude: float, longitude: float, **kwargs):\n",
    "    print(f\"NOAA get simple weather for '{latitude}, {longitude}'\")\n",
    "    n = NOAA()\n",
    "    description = False\n",
    "    fahrenheit_temp = 0\n",
    "    try:\n",
    "        observations = n.get_observations_by_lat_lon(latitude, longitude, num_of_stations=1)\n",
    "        for observation in observations:\n",
    "            description = observation[\"textDescription\"]\n",
    "            celcius_temp = observation[\"temperature\"][\"value\"]\n",
    "            if description:\n",
    "                break\n",
    "\n",
    "        fahrenheit_temp = (celcius_temp * 9 / 5) + 32\n",
    "\n",
    "        # fallback to temperature if no description in any of the observations\n",
    "        if fahrenheit_temp and not description:\n",
    "            description = fahrenheit_temp\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting NOAA weather: {e}\")\n",
    "\n",
    "    return description, fahrenheit_temp\n",
    "\n",
    "async def fetch_weather_from_api(\n",
    "    function_name, tool_call_id, args, llm, context, result_callback\n",
    "):\n",
    "    location = args[\"location\"]\n",
    "    latitude = float(args[\"latitude\"])\n",
    "    longitude = float(args[\"longitude\"])\n",
    "    print(f\"fetch_weather_from_api * location: {location}, lat & lon: {latitude}, {longitude}\")\n",
    "\n",
    "    if latitude and longitude:\n",
    "        description, fahrenheit_temp = await get_noaa_simple_weather(latitude, longitude)\n",
    "    else:\n",
    "        return await result_callback(\"Sorry, I don't recognize that location.\")\n",
    "\n",
    "    if not description:\n",
    "        await result_callback(\n",
    "            f\"I'm sorry, I can't get the weather for {location} right now. Can you ask again please?\"\n",
    "        )\n",
    "    else:\n",
    "        await result_callback(\n",
    "            f\"The weather in {location} is currently {round(fahrenheit_temp)} degrees and {description}.\"\n",
    "        )\n",
    "\n",
    "tools = [\n",
    "    ChatCompletionToolParam(\n",
    "        type=\"function\",\n",
    "        function={\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location for the weather request.\",\n",
    "                    },\n",
    "                    \"latitude\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Infer the latitude from the location. Supply latitude as a string. For example, '42.3601'.\",\n",
    "                    },\n",
    "                    \"longitude\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Infer the longitude from the location. Supply longitude as a string. For example, '-71.0589'.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"location\", \"latitude\", \"longitude\"],\n",
    "            },\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm.register_function(None, fetch_weather_from_api, start_callback=start_fetch_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fc5c4-e667-4ba7-bce4-de397cc40000",
   "metadata": {},
   "source": [
    "## Initialize the Context Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576f72b-556e-4218-92e3-a06bbd5bf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "context = OpenAILLMContext(messages, tools)\n",
    "context_aggregator = llm.create_context_aggregator(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8252bd2-62d9-43fc-9957-0c852215bc1b",
   "metadata": {},
   "source": [
    "Create pipeline to process speech into text with RIVA, send to NVIDIA NIM, then turn the NVIDIA NIM response text into speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8620a2-4caa-40c5-88d9-8aca2743157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        transport.input(),              # Transport user input\n",
    "        stt,                            # STT\n",
    "        context_aggregator.user(),      # User responses\n",
    "        llm,                            # LLM\n",
    "        tts,                            # TTS\n",
    "        transport.output(),             # Transport agent output\n",
    "        context_aggregator.assistant(), # Assistant spoken responses\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c588f-0c00-4414-984a-33da31e2803d",
   "metadata": {},
   "source": [
    "Create a PipelineTask to allow interruption while in conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbadb9a-9778-4f0f-910f-5c53d117e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.task import PipelineParams, PipelineTask\n",
    "\n",
    "task = PipelineTask(pipeline, PipelineParams(allow_interruptions=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4890ce7-6a1a-4f39-b6af-9a3335ad9fcf",
   "metadata": {},
   "source": [
    "Create a pipeline runner to manage the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e504ab-b889-4b6a-96a1-159d42a95833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "\n",
    "runner = PipelineRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162c265-39cd-49d1-beb6-fcf368572156",
   "metadata": {},
   "source": [
    "### Set event handlers\n",
    "- The `on_first_participant_joined` handler tells the agent to start the conversation when you join the call.\n",
    "- The `on_participant_left` handler sends an EndFrame which signals to terminate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2917234-efc6-440d-b427-ca4acab0b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.frames.frames import EndFrame\n",
    "\n",
    "@transport.event_handler(\"on_first_participant_joined\")\n",
    "async def on_first_participant_joined(transport, participant):\n",
    "    await task.queue_frames([context_aggregator.user().get_context_frame()])\n",
    "        \n",
    "@transport.event_handler(\"on_participant_left\")\n",
    "async def on_participant_left(transport, participant, reason):\n",
    "    print(f\"Participant left: {participant}\")\n",
    "    await task.queue_frame(EndFrame())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08998f8d-ac33-4b38-b10a-01691f81636a",
   "metadata": {},
   "source": [
    "## Run the Agent!\n",
    "\n",
    "NOTE: The first time you run the agent, it will load weights for a voice activity model into the local Python process. This will take 10-15 seconds. A permissions dialog will ask you to allow the browser to access your camera and microphone. Click yes to start talking to the agent. If you have any trouble with this, see [here](https://help.daily.co/en/articles/2525908-allow-camera-and-mic-access).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a411cb-d2c8-4446-be69-b391486e853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url to talk to the NVIDIA NIM Agent\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(f\"Navigate to: {DAILY_ROOM_URL}\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "await runner.run(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c5d71",
   "metadata": {},
   "source": [
    "### Suggested conversations:\n",
    "- *Learn.* Ask the agent about NVIDIA's developments in Agentic AI.\n",
    "- *Try tool calling.* As the agent about the weather.\n",
    "- *Observe the agent's context \"memory\".* After a few minutes of conversation, ask the agent what to recite the very first thing you said.\n",
    "\n",
    "To end the chat with the agent, leave the WebRTC call."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.12",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
