{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db60caf-a890-4e62-8255-62fd691cd6e6",
   "metadata": {},
   "source": [
    "# Voice Agent for Conversational AI with Pipecat\n",
    "In this notebook, we walk through how to craft and deploy a voice AI agent agent using Pipecat AI. We illustrate the basic Pipecat flow with the `meta/llama-3.3-70b-instruct`* LLM model and Riva for STT (Speech-To-Text) & TTS (Text-To-Speech). However, Pipecat is not opinionated and other models and STT/TTS services can easily be used. See [Pipecat documentation](https://docs.pipecat.ai/server/services/supported-services#supported-services) for other supported services.\n",
    "\n",
    "Pipecat AI is an open-source framework for building voice and multimodal conversational agents. Pipecat simplifies the complex voice-to-voice AI pipeline, and lets developers build AI capabilities easily and with Open Source, commercial, and custom models. See [Pipecat's Core Concepts](https://docs.pipecat.ai/getting-started/core-concepts) for a deep dive into how it works.\n",
    "\n",
    "The framework was developed by Daily, a company that has provided real-time video and audio communication infrastructure since 2016. It is fully vendor neutral and is not tightly coupled to Daily's infrastructure. That said, we do use it in this demo. Sign up for a Daily-agent API key [here](https://agents.daily.co/sign-up).\n",
    "\n",
    "> [Development Note]: *We are using \"meta/llama-3.3-70b-instruct\" for now because it works with tool calling, but can update/change this model at any time. It is a one line change in the notebook.\n",
    "\n",
    "Below is the architecture diagram\n",
    "\n",
    "![Architecture Diagram](./arch.png)\n",
    "\n",
    "A three-phase approach is used for Conversational AI Agent with Pipecat and NVIDIA NIM\n",
    "\n",
    "Phase 1 : User Input \n",
    "- Audio Processing with NVIDIA RIVA ASR with NIM\n",
    "\n",
    "Phase 2: User Content Aggregator with Pipecat and NVIDIA NIM\n",
    "- Custom processing with Pipecat \n",
    "- NVIDIA RIVA TTS with NIM\n",
    "\n",
    "Phase 3: Run the Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceb5b93",
   "metadata": {},
   "source": [
    "# Content Overview \n",
    "\n",
    "- [Prerequisites](#prerequisites)\n",
    "- [Initialize the User Input](#initialize-the-user-input)\n",
    "- [Initialize the Content Aggragtor](#initialize-the-context-aggregator) \n",
    "- [Run the Agent](#run-the-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4fa7d7-88fb-4b33-8145-ee1a91e58af1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### NGC API Key\n",
    "Prior to getting started, you will need to create API Keys for the NVIDIA API Catalog, Tavily, and Langchain.\n",
    "\n",
    "- NVIDIA API Catalog\n",
    "  1. Navigate to **[NVIDIA API Catalog](https://build.nvidia.com/explore/discover)**.\n",
    "  2. Select any model, such as llama-3.3-70b-instruct.\n",
    "  3. On the right panel above the sample code snippet, click on \"Get API Key\". This will prompt you to log in if you have not already.\n",
    "\n",
    "#### Export API Keys\n",
    "\n",
    "Save these API Keys as environment variables.\n",
    "\n",
    "First, set the NVIDIA API Key as the environment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d74dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4cf235",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "First we set our environment.\n",
    "\n",
    "We use Daily for transport, OpenAI for context aggregation, Riva for TTS & TTS, and Silero for VAD (Voice Activity Detection). If using different services, for example Cartesia for TTS, one would run `pip install \"pipecat-ai[cartesia]\"`.\n",
    "\n",
    "> [Development note]: We're installing from the github main branch here to ensure we have the latest improvements. By the time we address feedback we'll have a new release of Pipecat and just install the pipecat parts we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ea386",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/pipecat-ai/pipecat.git@main\"\n",
    "!pip install \"pipecat-ai[daily,openai,riva,silero]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979c5d1-97a9-42e7-9de2-88b7d31b1409",
   "metadata": {},
   "source": [
    "## Initialize the User Input\n",
    "\n",
    "Configure Daily transport for WebRTC communication\n",
    "- DAILY_SAMPLE_ROOM_URL: Where to connect (and where will navigate to to talk to our agent)\n",
    "- None: No authentication token needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb09c6-b9f7-409a-9ad4-742f494f6367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Url to talk to the NVIDIA NIM agent\n",
    "# Update to your sample room url after obtaining Daily-agent API key\n",
    "#### NOTE: if this is changed, the link in Step 11 markdown will no longer work.\n",
    "\n",
    "DAILY_SAMPLE_ROOM_URL=\"https://pc-34b1bdc94a7741719b57b2efb82d658e.daily.co/prod-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1be9b-e052-4430-b7e1-d7bf57a5ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.audio.vad.silero import SileroVADAnalyzer\n",
    "from pipecat.transports.services.daily import DailyParams, DailyTransport\n",
    "\n",
    "transport = DailyTransport(\n",
    "    DAILY_SAMPLE_ROOM_URL,\n",
    "    None,\n",
    "    \"NVIDIA NIM Agent\",\n",
    "    DailyParams(\n",
    "        audio_out_enabled=True,\n",
    "        vad_enabled=True,\n",
    "        vad_analyzer=SileroVADAnalyzer(),\n",
    "        vad_audio_passthrough=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8506527e-b84c-49e1-8af4-223fdb33f582",
   "metadata": {},
   "source": [
    "#### Initialize the LLM, RIVA services with NVIDIA NIM\n",
    "\n",
    "you can customize the different LLM `model`, that works with RIVA ASR and TTS services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623d77d5-c183-43d0-980d-fd99a2836365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pipecat.services.nim import NimLLMService\n",
    "from pipecat.services.riva import FastPitchTTSService, ParakeetSTTService\n",
    "\n",
    "stt = ParakeetSTTService(api_key=os.getenv(\"NVIDIA_API_KEY\"))\n",
    "\n",
    "llm = NimLLMService(\n",
    "    api_key=os.getenv(\"NVIDIA_API_KEY\"), model=\"meta/llama-3.3-70b-instruct\"\n",
    ")\n",
    "\n",
    "tts = FastPitchTTSService(api_key=os.getenv(\"NVIDIA_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac150732-cbb4-4c70-8d31-cab5ae51b5fb",
   "metadata": {},
   "source": [
    "Now it's time to Define LLM prompt as needed but you can always edit the prompt as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d884775-c4c0-49eb-b502-d4c855cc8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way and make a weather pun if it is possible.\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9923291b-4823-46f9-88f6-e88b42bf3191",
   "metadata": {},
   "source": [
    "#### Define tool calling function for weather queries \n",
    "\n",
    "Here we use the classic \"get_weather\" example. We use OpenAI's ChatCompletionToolParam and register the function with the llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e13ce0-5077-4ccc-be54-12932d816542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.types.chat import ChatCompletionToolParam\n",
    "from pipecat.frames.frames import TextFrame\n",
    "\n",
    "\n",
    "async def start_fetch_weather(function_name, llm, context):\n",
    "    await llm.push_frame(TextFrame(\"Let me check on that.\"))\n",
    "    print(f\"Starting fetch_weather_from_api with function_name: {function_name}\")\n",
    "\n",
    "async def fetch_weather_from_api(function_name, tool_call_id, args, llm, context, result_callback):\n",
    "    await result_callback({\"conditions\": \"nice\", \"temperature\": \"75\"})\n",
    "\n",
    "tools = [\n",
    "            ChatCompletionToolParam(\n",
    "                type=\"function\",\n",
    "                function={\n",
    "                    \"name\": \"get_current_weather\",\n",
    "                    \"description\": \"Returns the current weather at a location, if one is specified, and defaults to the user's location.\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The location to find the weather of, or if not provided, it's the default location.\",\n",
    "                            },\n",
    "                            \"format\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                                \"description\": \"Whether to use SI or USCS units (celsius or fahrenheit).\",\n",
    "                            },\n",
    "                        },\n",
    "                        \"required\": [\"location\", \"format\"],\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "        ]\n",
    "\n",
    "llm.register_function(None, fetch_weather_from_api, start_callback=start_fetch_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fc5c4-e667-4ba7-bce4-de397cc40000",
   "metadata": {},
   "source": [
    "## Initialize the Context Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576f72b-556e-4218-92e3-a06bbd5bf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext\n",
    "\n",
    "context = OpenAILLMContext(messages, tools)\n",
    "context_aggregator = llm.create_context_aggregator(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752c614-a65d-4c61-965f-26d7b46f8153",
   "metadata": {},
   "source": [
    "Create pipeline to process speech into text with RIVA, send to NVIDIA NIM, then turn the NVIDIA NIM response text into speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8620a2-4caa-40c5-88d9-8aca2743157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        transport.input(),              # Transport user input\n",
    "        stt,                            # STT\n",
    "        context_aggregator.user(),      # User responses\n",
    "        llm,                            # LLM\n",
    "        tts,                            # TTS\n",
    "        transport.output(),             # Transport agent output\n",
    "        context_aggregator.assistant(), # Assistant spoken responses\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c588f-0c00-4414-984a-33da31e2803d",
   "metadata": {},
   "source": [
    "Create a PipelineTask to allow interruption while in conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbadb9a-9778-4f0f-910f-5c53d117e593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.task import PipelineParams, PipelineTask\n",
    "\n",
    "task = PipelineTask(pipeline, PipelineParams(allow_interruptions=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4890ce7-6a1a-4f39-b6af-9a3335ad9fcf",
   "metadata": {},
   "source": [
    "Create a pipeline runner to manage the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e504ab-b889-4b6a-96a1-159d42a95833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.pipeline.runner import PipelineRunner\n",
    "\n",
    "runner = PipelineRunner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162c265-39cd-49d1-beb6-fcf368572156",
   "metadata": {},
   "source": [
    "#### Set Function call event handlers\n",
    "There are two handlers here \n",
    "\n",
    "First one `on_first_participant_joined` handler tells the agent to start the conversation when you join the call.  \n",
    "\n",
    "Second one `on_participant_left` handler sends an EndFrame which signals to terminate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2917234-efc6-440d-b427-ca4acab0b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipecat.frames.frames import LLMMessagesFrame, EndFrame\n",
    "\n",
    "@transport.event_handler(\"on_first_participant_joined\")\n",
    "async def on_first_participant_joined(transport, participant):\n",
    "    # Kick off the conversation.\n",
    "    messages.append({\"role\": \"system\", \"content\": \"Please introduce yourself to the user and deliver a weather fact.\"})\n",
    "    await task.queue_frames([LLMMessagesFrame(messages)])\n",
    "\n",
    "@transport.event_handler(\"on_participant_left\")\n",
    "async def on_participant_left(transport, participant, reason):\n",
    "    print(f\"Participant left: {participant}\")\n",
    "    await task.queue_frame(EndFrame())   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08998f8d-ac33-4b38-b10a-01691f81636a",
   "metadata": {},
   "source": [
    "## Run the Agent!\n",
    "\n",
    "`NOTE:` \n",
    "    The first time you run the agent, it will load weights for a voice activity model into the local Python process. This will take 10-15 seconds. A permissions dialog will ask you to allow the browser to access your camera and microphone. Click yes to start talking to the agent. If you have any trouble with this, see [here](https://help.daily.co/en/articles/2525908-allow-camera-and-mic-access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a411cb-d2c8-4446-be69-b391486e853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await runner.run(task)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
